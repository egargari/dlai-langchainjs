{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to LangChain.js\n",
    "\n",
    "Welcome! This short course will introduce you to [LangChain.js](https://github.com/langchain-ai/langchainjs), a framework for building large language model (LLM) powered, context-aware, reasoning applications. By the end of the course, you'll learn all the concepts you need to create your own version of a popular example of this type of app: a \"chat with data\" chain that lets you ask questions about a document's contents using natural language.\n",
    "\n",
    "This course follows on from the previous DeepLearning.ai courses on the Python version of LangChain. Much like LangChain.js itself, it's intended for web developers and others in the broader JavaScript ecosystem interested in building with LLMs, and will dive more deeply into features like streaming and integration with standard web/JavaScript APIs.\n",
    "\n",
    "**Note:** This notebook uses the [Deno](https://deno.com/) Jupyter kernel, and has slightly different import statements from those you may be familiar with in Node and web runtimes. You can convert them to Node/web imports by swapping `npm:langchain@0.0.178` for `langchain`. To run these course notebooks locally, see [these setup instructions](https://docs.deno.com/runtime/manual/tools/jupyter).\n",
    "\n",
    "**Note:** Throughout this course, we'll link to explorable traces in [LangSmith](https://smith.langchain.com/), LangChain's accompanying observability platform, that illustrate how the different example chains work. If you're following along locally, you'll need to set a few environment variables [as documented here](https://docs.smith.langchain.com/).\n",
    "\n",
    "## Building Blocks: LangChain Expression Language\n",
    "\n",
    "At the core of LangChain is LangChain Expression Language (LCEL). LCEL is a composable syntax for chaining LangChain modules together. Objects that are compatible with LCEL are called `Runnables`.\n",
    "\n",
    "To show how this works, we'll compose a simple chain containing two of LangChain's primary modules: a prompt template that formats our input and an OpenAI chat model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { ChatPromptTemplate } from \"npm:langchain@0.0.178/prompts\";\n",
    "import { ChatOpenAI } from \"npm:langchain@0.0.178/chat_models/openai\";\n",
    "\n",
    "Deno.env.set(\"OPENAI_API_KEY\", \"\");\n",
    "Deno.env.set(\"LANGCHAIN_TRACING_V2\", \"true\");\n",
    "Deno.env.set(\"LANGCHAIN_SESSION\", \"\");\n",
    "Deno.env.set(\"LANGCHAIN_API_KEY\", \"\");\n",
    "\n",
    "const prompt = ChatPromptTemplate.fromTemplate(`What are three good names for a company that makes {product}?`);\n",
    "\n",
    "const model = new ChatOpenAI({});\n",
    "\n",
    "const chain = prompt.pipe(model);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prompt template will take input variables and inject them into the template. The chain then pipes the properly formatted result into the chat model as input. We can run the LCEL chain as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage {\n",
       "  lc_serializable: \u001b[33mtrue\u001b[39m,\n",
       "  lc_kwargs: {\n",
       "    content: \u001b[32m\"1. ChromaSock\\n2. VividThread\\n3. KaleidoSock\"\u001b[39m,\n",
       "    additional_kwargs: { function_call: \u001b[90mundefined\u001b[39m }\n",
       "  },\n",
       "  lc_namespace: [ \u001b[32m\"langchain\"\u001b[39m, \u001b[32m\"schema\"\u001b[39m ],\n",
       "  content: \u001b[32m\"1. ChromaSock\\n2. VividThread\\n3. KaleidoSock\"\u001b[39m,\n",
       "  name: \u001b[90mundefined\u001b[39m,\n",
       "  additional_kwargs: { function_call: \u001b[90mundefined\u001b[39m }\n",
       "}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await chain.invoke({\n",
    "  product: \"colorful socks\",\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can see that the result is an `AIMessage` with three good names for a company that makes colorful socks.\n",
    "\n",
    "If we just want the output as a raw string rather than an AI message response, we can add a third step to our chain: an output parser!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\"1. Delicieux Delights\\n2. Gourmet Galettes\\n3. Decadent Crumbs\"\u001b[39m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import { StringOutputParser } from \"npm:langchain@0.0.178/schema/output_parser\";\n",
    "\n",
    "const outputParser = new StringOutputParser();\n",
    "\n",
    "const nameGenerationChain = prompt.pipe(model).pipe(outputParser);\n",
    "\n",
    "await nameGenerationChain.invoke({\n",
    "  product: \"fancy cookies\",\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see a LangSmith trace of the above example [here](https://smith.langchain.com/public/02c0469b-f3b4-4681-b006-da74df897dfa/r).\n",
    "\n",
    "These three pieces form the core of many more complicated chains.\n",
    "\n",
    "## Streaming\n",
    "\n",
    "One of the many advantages to LCEL is that chains composed in this fashion get certain methods automatically. One useful one is `.stream()`, which returns output from the chain in an iterable stream. Because LLM responses often take a long time to finish, streaming is useful in situations where showing feedback quickly is important.\n",
    "\n",
    "Here's an example with the chain we just composed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1\n",
      ".\n",
      " Rob\n",
      "o\n",
      "Tech\n",
      "\n",
      "\n",
      "2\n",
      ".\n",
      " Mech\n",
      "Wave\n",
      "\n",
      "\n",
      "3\n",
      ".\n",
      " Fut\n",
      "ura\n",
      "B\n",
      "ots\n",
      "\n"
     ]
    }
   ],
   "source": [
    "const stream = await nameGenerationChain.stream({\n",
    "  product: \"really cool robots\",\n",
    "});\n",
    "\n",
    "for await (const chunk of stream) {\n",
    "  console.log(chunk);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, the `model` emits partial message chunks, and the output parser transforms those streamed chunks from the model as they are generated, resulting in immediate string output.\n",
    "\n",
    "This stream is a special `ReadableStream` that implements the iterator interface and can be passed directly to e.g. a `fetch` built-in `Response` object and returned in popular web frameworks like Next.js."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response {\n",
       "  body: IterableReadableStream { locked: false },\n",
       "  bodyUsed: false,\n",
       "  headers: Headers {},\n",
       "  ok: true,\n",
       "  redirected: false,\n",
       "  status: 200,\n",
       "  statusText: \"\",\n",
       "  url: \"\"\n",
       "}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "const stream = await nameGenerationChain.stream({\n",
    "  product: \"superb owls\",\n",
    "});\n",
    "\n",
    "new Response(stream);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chaining with `RunnableMap`\n",
    "\n",
    "While sequentially chaining calls together like this can be useful on its own, more complex chains often need to combine inputs from different sources and steps. For example, let's say we want to take the company name outputs generated by the above chain and pass it as one input of several into another chain that picks the best one. One way to do this is to add a `RunnableMap` to the sequence. We also declare the sequence using the alternate `RunnableSequence` factory method for readability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { RunnableSequence, RunnableMap } from \"npm:langchain@0.0.178/schema/runnable\";\n",
    "\n",
    "const finalPrompt = ChatPromptTemplate.fromTemplate(`Pick the best name from {names} based on {final_criteria} and explain why.`);\n",
    "\n",
    "const combinedChain = RunnableSequence.from([\n",
    "  RunnableMap.from({\n",
    "    names: nameGenerationChain,\n",
    "    final_criteria: (input) => input.final_criteria,\n",
    "  }),\n",
    "  finalPrompt,\n",
    "  model,\n",
    "  new StringOutputParser(),\n",
    "]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each property in the `RunnableMap` gets called in parallel with the input from the previous step. In the above example, since the `RunnableMap` is the first step, it is called with the input to the overall chain. The output of the `RunnableMap` is an object with the result of that call. Since we want to pass `final_criteria` through to `finalPrompt`, we simply extract the property from the input.\n",
    "\n",
    "We can see the result below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\"From a sustainability perspective, the best name would be WoodWheelz.\\n\"\u001b[39m +\n",
       "  \u001b[32m\"\\n\"\u001b[39m +\n",
       "  \u001b[32m\"WoodWheelz implies the use of\"\u001b[39m... 598 more characters"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await combinedChain.invoke({\n",
    "  product: \"wooden cars\",\n",
    "  final_criteria: \"sustainability\",\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This [LangSmith trace](https://smith.langchain.com/public/ed4d767a-49d5-427c-97fe-fc2dc3f5aba5/r) shows what's going on under the hood - the `nameGenerationChain` we defined above generated three good names for a company that makes wooden cars, and the output was passed to our `finalPrompt` along with the `final_criteria` we originally passed to make a final judgement of the best name."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deno",
   "language": "typescript",
   "name": "deno"
  },
  "language_info": {
   "file_extension": ".ts",
   "mimetype": "text/x.typescript",
   "name": "typescript",
   "nb_converter": "script",
   "pygments_lexer": "typescript",
   "version": "5.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
