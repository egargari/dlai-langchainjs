{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to LangChain.js\n",
    "\n",
    "Welcome! This short course will introduce you to [LangChain.js](https://github.com/langchain-ai/langchainjs), a framework for building large language model (LLM) powered, context-aware, reasoning applications. By the end of the course, you'll learn all the concepts you need to create your own version of a popular example of this type of app: a \"chat with data\" chain that lets you ask questions about a document's contents using natural language.\n",
    "\n",
    "This course follows on from the previous DeepLearning.ai courses on the Python version of LangChain. Much like LangChain.js itself, it's intended for web developers and others in the broader JavaScript ecosystem interested in building with LLMs, and will dive more deeply into features like streaming and integration with standard web/JavaScript APIs.\n",
    "\n",
    "**Note:** This notebook uses the [Deno](https://deno.com/) Jupyter kernel, and has slightly different import statements from those you may be familiar with in Node and web runtimes. You can convert them to Node/web imports by swapping `npm:langchain@0.0.178` for `langchain`. To run these course notebooks locally, see [these setup instructions](https://docs.deno.com/runtime/manual/tools/jupyter).\n",
    "\n",
    "**Note:** Throughout this course, we'll link to explorable traces in [LangSmith](https://smith.langchain.com/), an LLM-focused observability platform by the company behind the open source framework, that illustrate how the different example chains work. If you're following along locally, you'll need to set a few environment variables [as documented here](https://docs.smith.langchain.com/).\n",
    "\n",
    "## Building Blocks: LLMs\n",
    "\n",
    "Let's start with one of the most fundamental pieces of LangChain: the language model. LangChain includes two different types of language models: \n",
    "\n",
    "1. LLMs, which take a string as input and returns a string. An example of this is OpenAI's `text-davinci-003`, also known as GPT-3.\n",
    "2. Chat Models, which take a list of messages as input and return a message. An example of this is OpenAI's `gpt-4`.\n",
    "\n",
    "As strings, LLMs inputs and outputs are easy to visualize, so let's look at what calling a chat model directly looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage {\n",
       "  lc_serializable: \u001b[33mtrue\u001b[39m,\n",
       "  lc_kwargs: {\n",
       "    content: \u001b[32m\"Why don't scientists trust atoms? \\n\\nBecause they make up everything!\"\u001b[39m,\n",
       "    additional_kwargs: { function_call: \u001b[90mundefined\u001b[39m }\n",
       "  },\n",
       "  lc_namespace: [ \u001b[32m\"langchain\"\u001b[39m, \u001b[32m\"schema\"\u001b[39m ],\n",
       "  content: \u001b[32m\"Why don't scientists trust atoms? \\n\\nBecause they make up everything!\"\u001b[39m,\n",
       "  name: \u001b[90mundefined\u001b[39m,\n",
       "  additional_kwargs: { function_call: \u001b[90mundefined\u001b[39m }\n",
       "}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Deno.env.set(\"OPENAI_API_KEY\", \"\");\n",
    "// Deno.env.set(\"LANGCHAIN_TRACING_V2\", \"true\");\n",
    "// Deno.env.set(\"LANGCHAIN_SESSION\", \"\");\n",
    "// Deno.env.set(\"LANGCHAIN_API_KEY\", \"\");\n",
    "\n",
    "import { ChatOpenAI } from \"npm:langchain@0.0.178/chat_models/openai\";\n",
    "import { HumanMessage } from \"npm:langchain@0.0.178/schema\";\n",
    "\n",
    "const model = new ChatOpenAI({});\n",
    "\n",
    "await model.invoke([\n",
    "  new HumanMessage(\"Tell me a joke.\"),\n",
    "]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we pass in an array with a single `HumanMessage` as input, and receive a single `AIMessage` as output. Messages have a `content` field containing the text value of the message, and an associated `role` that corresponds to the entity sending the message. \n",
    "\n",
    "**Note:** While this course will primarily use OpenAI's `gpt-3.5-turbo` chat model, LangChain supports models from many different providers, and you can try swapping the provided class in any of the code examples.\n",
    "\n",
    "## Building Blocks: Prompt Templates\n",
    "\n",
    "While calling models in isolation can be useful, it is often more convenient to factor out the logic behind model inputs into a reusable, parameterized component. For this purpose, LangChain includes prompt templates, which are responsible for formatting user input for later model calls. Input variables are denoted by curly braces within the template, and will be substituted into the final formatted value. \n",
    "\n",
    "Prompt templates are also useful for smoothing over some of the differences in model input types. Below, we construct a prompt template directly from a string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { ChatPromptTemplate } from \"npm:langchain@0.0.178/prompts\";\n",
    "\n",
    "const prompt = ChatPromptTemplate.fromTemplate(`What are three good names for a company that makes {product}?`);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we can use this prompt template to generate both string input for an LLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\"Human: What are three good names for a company that makes colorful socks?\"\u001b[39m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await prompt.format({\n",
    "  product: \"colorful socks\",\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or a message array for chat models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\n",
       "  HumanMessage {\n",
       "    lc_serializable: \u001b[33mtrue\u001b[39m,\n",
       "    lc_kwargs: {\n",
       "      content: \u001b[32m\"What are three good names for a company that makes colorful socks?\"\u001b[39m,\n",
       "      additional_kwargs: {}\n",
       "    },\n",
       "    lc_namespace: [ \u001b[32m\"langchain\"\u001b[39m, \u001b[32m\"schema\"\u001b[39m ],\n",
       "    content: \u001b[32m\"What are three good names for a company that makes colorful socks?\"\u001b[39m,\n",
       "    name: \u001b[90mundefined\u001b[39m,\n",
       "    additional_kwargs: {}\n",
       "  }\n",
       "]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await prompt.formatMessages({\n",
    "  product: \"colorful socks\",\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though we can pass these formatted values directly into a model, there's a more elegant way to use prompts and models together that we'll go over next.\n",
    "\n",
    "## Building Blocks: LangChain Expression Language\n",
    "\n",
    "LangChain Expression Language (LCEL) is a composable syntax for chaining LangChain modules together. Objects that are compatible with LCEL are called `Runnables`.\n",
    "\n",
    "We can construct a simple chain from the prompt and model we declared above like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "const chain = prompt.pipe(model);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input of the chain is the same as the first step in the sequence, which in this case is an object with a single `product` property. The prompt template is invoked with this input, then passes the properly formatted result as input into the next step of the chain, the chat model. Here's what it looks like in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage {\n",
       "  lc_serializable: \u001b[33mtrue\u001b[39m,\n",
       "  lc_kwargs: {\n",
       "    content: \u001b[32m\"1. ChromaSockz\\n2. RainbowThreads\\n3. VibrantFeet\"\u001b[39m,\n",
       "    additional_kwargs: { function_call: \u001b[90mundefined\u001b[39m }\n",
       "  },\n",
       "  lc_namespace: [ \u001b[32m\"langchain\"\u001b[39m, \u001b[32m\"schema\"\u001b[39m ],\n",
       "  content: \u001b[32m\"1. ChromaSockz\\n2. RainbowThreads\\n3. VibrantFeet\"\u001b[39m,\n",
       "  name: \u001b[90mundefined\u001b[39m,\n",
       "  additional_kwargs: { function_call: \u001b[90mundefined\u001b[39m }\n",
       "}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await chain.invoke({\n",
    "  product: \"colorful socks\",\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the result is an `AIMessage` with three good names for a company that makes colorful socks.\n",
    "\n",
    "## Building Blocks: Output Parser\n",
    "\n",
    "The final consideration we'll go over in this section is formatting our output. For example, it is often easier to work with the raw string value of a chat model's output rather than an AI message. The LangChain abstraction for this is called an output parser. \n",
    "\n",
    "Below, we redefine our chain with an output parser that coerces the message output from the chat model into a string as a third and final step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { StringOutputParser } from \"npm:langchain@0.0.178/schema/output_parser\";\n",
    "\n",
    "const outputParser = new StringOutputParser();\n",
    "\n",
    "const nameGenerationChain = prompt.pipe(model).pipe(outputParser);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, if we invoke the chain, we can see that the output is a raw string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\"1. Dolce Delights\\n2. Gourmet Crumbles\\n3. Elegance Bites\"\u001b[39m"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await nameGenerationChain.invoke({\n",
    "  product: \"fancy cookies\",\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see a LangSmith trace of the above example [here](https://smith.langchain.com/public/02c0469b-f3b4-4681-b006-da74df897dfa/r).\n",
    "\n",
    "These three pieces form the core of many more complicated chains.\n",
    "\n",
    "## Streaming\n",
    "\n",
    "One of the many advantages to LCEL is that chains composed in this fashion get certain methods automatically. One useful one is `.stream()`, which returns output from the chain in an iterable stream. Because LLM responses often take a long time to finish, streaming is useful in situations where showing feedback quickly is important.\n",
    "\n",
    "Here's an example with the chain we just composed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1\n",
      ".\n",
      " Rob\n",
      "o\n",
      "Tech\n",
      " Innov\n",
      "ations\n",
      "\n",
      "\n",
      "2\n",
      ".\n",
      " Cyber\n",
      "tron\n",
      " Robotics\n",
      "\n",
      "\n",
      "3\n",
      ".\n",
      " Fut\n",
      "u\n",
      "Robot\n",
      "ics\n",
      "\n"
     ]
    }
   ],
   "source": [
    "const stream = await nameGenerationChain.stream({\n",
    "  product: \"really cool robots\",\n",
    "});\n",
    "\n",
    "for await (const chunk of stream) {\n",
    "  console.log(chunk);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, the `model` emits partial message chunks, and the output parser transforms those streamed chunks from the model as they are generated, resulting in immediate string output.\n",
    "\n",
    "This stream is a special `ReadableStream` that implements the iterator interface and can be passed directly to e.g. a `fetch` built-in `Response` object and returned in popular web frameworks like Next.js."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response {\n",
       "  body: IterableReadableStream { locked: false },\n",
       "  bodyUsed: false,\n",
       "  headers: Headers {},\n",
       "  ok: true,\n",
       "  redirected: false,\n",
       "  status: 200,\n",
       "  statusText: \"\",\n",
       "  url: \"\"\n",
       "}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "const stream = await nameGenerationChain.stream({\n",
    "  product: \"superb owls\",\n",
    "});\n",
    "\n",
    "new Response(stream);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chaining with `RunnableMap`\n",
    "\n",
    "While sequentially chaining calls together like this can be useful on its own, more complex chains often need to combine inputs from different sources and steps. For example, let's say we want to take the company name outputs generated by the above chain and pass it as one input of several into another chain that picks the best one. One way to do this is to add a `RunnableMap` to the sequence. We also declare the sequence using the alternate `RunnableSequence.from()` method for readability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { RunnableSequence, RunnableMap } from \"npm:langchain@0.0.178/schema/runnable\";\n",
    "\n",
    "const finalPrompt = ChatPromptTemplate.fromTemplate(`Pick the best name from {names} based on {final_criteria} and explain why.`);\n",
    "\n",
    "const combinedChain = RunnableSequence.from([\n",
    "  RunnableMap.from({\n",
    "    names: nameGenerationChain,\n",
    "    final_criteria: (input) => input.final_criteria,\n",
    "  }),\n",
    "  finalPrompt,\n",
    "  model,\n",
    "  new StringOutputParser(),\n",
    "]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each property in the `RunnableMap` gets called in parallel with the input from the previous step. In the above example, since the `RunnableMap` is the first step, it is called with the input to the overall chain. The output of the `RunnableMap` is an object with the result of that call. Since we want to pass `final_criteria` through to `finalPrompt`, we simply extract the property from the input.\n",
    "\n",
    "We can see the result below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m'The best name among the three options based on sustainability would be \"EcoCarve Automotive.\" \\n'\u001b[39m +\n",
       "  \u001b[32m\"\\n\"\u001b[39m +\n",
       "  \u001b[32m\"This\"\u001b[39m... 496 more characters"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await combinedChain.invoke({\n",
    "  product: \"wooden cars\",\n",
    "  final_criteria: \"sustainability\",\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This [LangSmith trace](https://smith.langchain.com/public/ed4d767a-49d5-427c-97fe-fc2dc3f5aba5/r) shows what's going on under the hood - the `nameGenerationChain` we defined above generated three good names for a company that makes wooden cars, and the output was passed to our `finalPrompt` along with the `final_criteria` we originally passed to make a final judgement of the best name."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deno",
   "language": "typescript",
   "name": "deno"
  },
  "language_info": {
   "file_extension": ".ts",
   "mimetype": "text/x.typescript",
   "name": "typescript",
   "nb_converter": "script",
   "pygments_lexer": "typescript",
   "version": "5.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
